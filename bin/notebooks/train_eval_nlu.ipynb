{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kobrue02/evaluating-llm-generated-nlu-data/blob/main/bin/notebooks/train_eval_nlu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "wBUAwpMtG5Xf"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/kobrue02/evaluating-llm-generated-nlu-data/\n",
        "%cd evaluating-llm-generated-nlu-data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "f1S3EwEuG5Xg"
      },
      "outputs": [],
      "source": [
        "from bin.framework.nlu_model import IntentClassifier\n",
        "from bin.utils.methods import *\n",
        "from sklearn.neural_network import MLPClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "8_cN5cpZG5Xg"
      },
      "outputs": [],
      "source": [
        "model = IntentClassifier(model=MLPClassifier())\n",
        "datasets = [\n",
        "    \"zero_shot_simple_data\",\n",
        "    \"one_shot_simple_data\",\n",
        "    \"few_shot_simple_data\",\n",
        "    \"chain_of_thought_simple_data\",\n",
        "    \"persona_based_prompt_s1_data\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "LHVZ-NSAG5Xg"
      },
      "outputs": [],
      "source": [
        "reports = {}\n",
        "for fname in datasets:\n",
        "    df = load_df(fname)\n",
        "    train_df, test_df = model.split_dataset(df)\n",
        "    model.fit(train_df)\n",
        "    report = model.evaluate(test_df)\n",
        "    reports[fname] = report\n",
        "    model.reset()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "Z_Y9lu5twvH4"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stacked_bar_chart(reports):\n",
        "    # Create a dictionary to store the weighted averages\n",
        "    weighted_averages = {}\n",
        "\n",
        "    for dataset_name, report in reports.items():\n",
        "        weighted_averages[dataset_name] = {}\n",
        "        for intent, metrics in report.items():\n",
        "            if intent not in {'accuracy', 'macro avg', 'weighted avg'}:\n",
        "                weighted_averages[dataset_name][intent] = {\n",
        "                    'precision': metrics['precision'],\n",
        "                    'recall': metrics['recall'],\n",
        "                    'f1-score': metrics['f1-score']\n",
        "                }\n",
        "\n",
        "    # Convert the weighted_averages dictionary to a Pandas DataFrame\n",
        "    df_list = []\n",
        "    for dataset_name, intents in weighted_averages.items():\n",
        "        for intent, metrics in intents.items():\n",
        "            df_list.append({\n",
        "                'dataset': dataset_name,\n",
        "                'intent': intent,\n",
        "                'precision': metrics['precision'],\n",
        "                'recall': metrics['recall'],\n",
        "                'f1-score': metrics['f1-score']\n",
        "            })\n",
        "    df = pd.DataFrame(df_list)\n",
        "\n",
        "    # Generate individual plots\n",
        "    for dataset_name in weighted_averages.keys():\n",
        "        dataset_df = df[df['dataset'] == dataset_name]\n",
        "        dataset_df = dataset_df.set_index('intent')[['precision', 'recall', 'f1-score']]\n",
        "\n",
        "        # Create a new figure for each dataset\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        dataset_df.plot(kind='bar', stacked=True, colormap=\"Set3\")\n",
        "\n",
        "        plt.title(f'Metrics per intent for {dataset_name}')\n",
        "        plt.ylabel('Score')\n",
        "        plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
        "\n",
        "        # Add a horizontal line with the weighted average F1 score\n",
        "        weighted_avg_f1 = df[df['dataset'] == dataset_name]['f1-score'].mean()\n",
        "        plt.axhline(y=weighted_avg_f1, color='b', linestyle='-')\n",
        "        plt.xticks([])\n",
        "\n",
        "        # Save the figure\n",
        "        plt.savefig(f'output/{dataset_name}.png', bbox_inches='tight')\n",
        "        plt.close()  # Close the figure to free memory\n",
        "\n",
        "    print(\"Plots saved successfully!\")"
      ],
      "metadata": {
        "id": "8CiPtOhnv5BM"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stacked_bar_chart(reports)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "5LhW2dYs0ce0",
        "outputId": "16b9c7b7-0294-4bf2-efa8-bbf4d526025b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plots saved successfully!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def heatmap(reports):\n",
        "  for dataset_name, report in reports.items():\n",
        "    # Convert report to DataFrame\n",
        "    df_report = pd.DataFrame(report).transpose()\n",
        "    # Plot heatmap\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(df_report.iloc[:-1, :3], annot=True, cmap='Blues', fmt='.2f')\n",
        "    plt.title(f'{dataset_name.title()} Classification Report Heatmap')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "7JX9pKJYxGIj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "heatmap(reports)"
      ],
      "metadata": {
        "id": "CVxxxlCnzZk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bin.utils.read_datasets import read_sipgate_dataset\n",
        "from bin.utils.clean_sipgate_dataset import clean_sipgate_dataset\n",
        "\n",
        "golden_df = clean_sipgate_dataset(read_sipgate_dataset())\n",
        "# subset with 25 queries per intent\n",
        "golden_df = golden_df.groupby('intent').sample(n=25, random_state=42)\n",
        "golden_df.head()"
      ],
      "metadata": {
        "id": "Oj0zzgfgN32a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = IntentClassifier(model=MLPClassifier())\n",
        "train_df, test_df = model.split_dataset(df)\n",
        "model.fit(train_df)\n",
        "report = model.classification_report(test_df)\n",
        "print(report)"
      ],
      "metadata": {
        "id": "X4kS_B0rPEwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "report_dict = model.evaluate(test_df)\n",
        "stacked_bar_chart({\n",
        "    \"sipgate\": report_dict\n",
        "})\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Fc-zhnnKRK_v",
        "outputId": "3980ed7b-eed3-45a9-d9ff-4750d7998dea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plots saved successfully!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}